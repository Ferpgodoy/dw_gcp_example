
# Project: Data Warehouse with Airflow, Astro CLI, and GCP

This project demonstrates building a data pipeline using Apache Airflow with Astro CLI, Docker, and Google Cloud Platform (GCP).  
The goal is to orchestrate data ingestion from APIs, store data in Google Cloud Storage (GCS), and perform layered transformations in BigQuery following the Medallion Architecture (Bronze, Silver, and Gold).

## üöÄ Technologies Used

- [Apache Airflow](https://airflow.apache.org/) with [Astro CLI](https://www.astronomer.io/docs/astro/cli/install-cli/)
- [Docker](https://www.docker.com/)
- [Google Cloud Platform (GCP)](https://cloud.google.com/)
  - Google Cloud Storage (GCS)
  - BigQuery
- [Terraform](https://developer.hashicorp.com/terraform)

## üìÅ Project Structure

```
projeto_1_dw/.astro
‚îú‚îÄ‚îÄ .astro                    # Astro configs
‚îú‚îÄ‚îÄ .github                   # Github Actions, with workflows for stage and prd
‚îÇ   ‚îî‚îÄ‚îÄ workflows/  
‚îú‚îÄ‚îÄ dags/                     # Airflow DAGs
‚îÇ   ‚îî‚îÄ‚îÄ tasks/                # Contains Airflow Tasks
‚îú‚îÄ‚îÄ include/  
‚îÇ   ‚îú‚îÄ‚îÄ python_scripts/           # Helper Python scripts
‚îÇ   ‚îÇ   ‚îÇ‚îÄ‚îÄ api_reader.py         # Generic function to read data from diverse APIs in JSON format
‚îÇ   ‚îÇ   ‚îÇ‚îÄ‚îÄ api_reader.py         # Function to delete blobs in GCS
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ execute_migrations.py # Function that executes SQL scripts in migrations/ to version database structure
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fetch_credentials.py  # Function that uses Terraform output to update GCP credentials on config/secrets
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gcs_uploader.py       # Diverse functions to upload different format files to GCS
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generate_fake_data.py # Diverse functions that generate different types of fake data, e.g. sales
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ read_sql_scripts.py   # Python script that reads SQL scripts parametized with '{}'
‚îÇ   ‚îî‚îÄ‚îÄ transformation/           # SQL scripts for transformations (Bronze, Silver, Gold)
‚îÇ       ‚îú‚îÄ‚îÄ bronze/               # Ingests raw data into BigQuery with minimal transformation
‚îÇ       ‚îú‚îÄ‚îÄ silver/               # Cleans and filters data, handles missing values, sets schema, and normalizes column names
‚îÇ       ‚îî‚îÄ‚îÄ gold/                 # Aggregates and derives final metrics used in dashboards or reports; adds business value
‚îú‚îÄ‚îÄ infra/
‚îÇ   ‚îÇ‚îÄ‚îÄ main.tf               # Defines the core infrastructure resources
‚îÇ   ‚îî‚îÄ‚îÄ variables.tf          # Declares reusable input variables and their types
‚îú‚îÄ‚îÄ migrations/               # DDL (Data Definition Language) SQL scripts to version database structure
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ secrets/              # Credential files (not versioned)
‚îú‚îÄ‚îÄ .env                      # environment variables (not versioned)
‚îú‚îÄ‚îÄ .env.example              # Example environment variables
‚îú‚îÄ‚îÄ Dockerfile                # Docker configuration
‚îú‚îÄ‚îÄ requirements.txt          # Project dependencies
‚îú‚îÄ‚îÄ requirements-ci.txt       # Project dependencies used in CI/CD
‚îî‚îÄ‚îÄ README.md                 # Project documentation
```

## ‚öôÔ∏è Prerequisites

Before starting, make sure you have the following installed and configured:

- [Astro CLI](https://www.astronomer.io/docs/astro/cli/install-cli/)
- [Docker](https://www.docker.com/get-started)
- An active project on [Google Cloud Platform (GCP)](https://cloud.google.com/)
- [Terraform](https://developer.hashicorp.com/terraform/install)

## ‚òÅÔ∏è GCP Setup

1. **Create a project in GCP**  
   If you don‚Äôt have one yet, create a new project in the [GCP Console](https://console.cloud.google.com/).

2. **Create a service account in GCP**:
   - Grant the necessary permissions:
      - roles/storage.admin
      - roles/bigquery.admin
      - roles/serviceusage.serviceUsageAdmin
   - Generate a JSON key for this account.
   

## üõ†Ô∏è Environment Setup

1. **Clone the repository**:

```bash
git clone https://github.com/Ferpgodoy/dw_gcp_example.git
cd dw_gcp_example
```

2. **Configure environment variables**:
   - Create a `.env` file based on `.env.example`.
   - Update the variables as needed.
   - Save the JSON key file generated for the GCP service account in the `config/secrets/` folder of the project as `gcp_credentials.json`.

4. **Setup backend for Terraform**:
```bash
# Load environment variables from .env file
export $(grep -v '^#' .env | xargs)

# Ensure the backend bucket exists
echo "Checking if bucket $TF_BACKEND_BUCKET exists..."
if ! gsutil ls -b gs://$TF_BACKEND_BUCKET > /dev/null 2>&1; then
  echo "Bucket does not exist. Creating..."
  gsutil mb -p $GCP_PROJECT_ID -l $REGION gs://$TF_BACKEND_BUCKET
else
  echo "Bucket already exists."
fi

# Create terraform.tfvars file
cat <<EOF > infra/terraform.tfvars
project_id  = "$GCP_PROJECT_ID"
gcp_key = "../config/secrets/gcp_credentials.json"
bucket = "$GCP_BUCKET_NAME"
region = "$REGION"
EOF
echo "File infra/terraform.tfvars created."

# Create backend.tf file
cat <<EOF > infra/backend.tf
terraform {
  backend "gcs" {
    bucket  = "$TF_BACKEND_BUCKET"
    prefix  = "infra"
  }
}
EOF
echo "File infra/backend.tf created."
```

4. **Create Resources with Terraform**:
```bash
# 1. access infra folder
cd infra

# 2. export GCP credentials location
export GOOGLE_APPLICATION_CREDENTIALS=../config/secrets/gcp_credentials.json

# 2. inicialize Terraform
terraform init

# 4. visualize creation plan
terraform plan -var-file=terraform.tfvars

# 5. apply Terraform resource creation
terraform apply -auto-approve -var-file=terraform.tfvars

## 6. back to root folder
cd ..
```

5. **Start the local environment using Astro CLI**:

```bash
astro dev start
```

This command will build and start the required Docker containers for Airflow.

6. **Access the Airflow UI**:
   - Go to [http://localhost:8080](http://localhost:8080)
   - Default credentials:
     - User: `admin`
     - Password: `admin`

## üì¨ Contact

For questions or suggestions, please reach out:

- GitHub: [@Ferpgodoy](https://github.com/Ferpgodoy)

## ‚òÅÔ∏è Raw Bucket Folder Structure

The raw data is stored in the GCS bucket with the following folder structure:

```
sales/
  ‚îî‚îÄ‚îÄ <execution_date>/
        ‚îî‚îÄ‚îÄ <timestamp>.json
```

- `<execution_date>`: The date when the DAG runs and extracts the data (format: YYYY-MM-DD).
- `<timestamp>.json`: The JSON file containing the sales data extracted, named with the timestamp of the extraction.

This structure helps organize the raw sales data by execution date and extraction time. 
It enables the project to retain historical files uploaded for each day, facilitating data auditing. 
Additionally, the latest file in each folder can always be used for backfilling purposes.

## üìÑ Running the DAGs

- The primary DAG, named `dag_sales_update`, is located in the `dags/dag_01.py` file.
- This DAG performs the following steps while keeping the pipeline lightweight by updating data for only one day at a time:
  1. Generates fake sales data for the current execution day.
  2. Stores the generated data in the `raw` GCS bucket, following the folder structure described above.
  3. Executes layered transformations in BigQuery, updating only the data for that specific day through the Bronze ‚Üí Silver ‚Üí Gold stages.
- In case of any issues, backfill runs can be triggered to reprocess and update data for multiple days as needed.
