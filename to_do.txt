pensar em novos exemplos de dados para construir arquitetura medalhÃ£o
configurar github actions (branches stage e main)
    configurar variables no github secrets
    yml:
        rodar .tf
        rodar execute_migrations.py
        criar env com variaveis do github secrets para o astronomer acessar
        rodar deploy Astronomer
construir testes/qualidade de dados
usar dbt

fontes de custo do projeto: GCS, BigQuery e Astronomer

ğŸ‡ºğŸ‡¸ English Version
ğŸš€ Kicking Off My GitHub Portfolio: GCP Data Warehouse Orchestrator

To start building my data engineering portfolio on GitHub, Iâ€™ve been working on a project that combines:

â€¢ ğŸŒ Google Cloud Platform (GCP)
â€¢ ğŸ§  BigQuery
â€¢ âš™ï¸ Apache Airflow (with Astronomer)
â€¢ ğŸ›ï¸ Medallion Architecture (Bronze, Silver, Gold)

The goal is to orchestrate end-to-end data pipelines â€” from ingesting API data into Google Cloud Storage (raw layer, multiple formats) to running parameterized SQL transformations in BigQuery across the medallion layers.

I started the project with a simple sales table model (using fake data), and I plan to evolve it gradually â€” including DBT in the next stages for better transformation management and modularization, and Terraform for more robust infrastructure deployment.

Python scripts support the ingestion and transformation logic, including:
ğŸ”¹ API data extraction
ğŸ”¹ Fake data generation
ğŸ”¹ GCS ingestion in multiple formats
ğŸ”¹ DDL migration control
ğŸ”¹ Parameterized SQL execution

ğŸ“‚ Repo: https://github.com/Ferpgodoy/dw_gcp_example

The project is still in progress, and Iâ€™d love to hear your thoughts.
ğŸ’¬ Iâ€™m open to feedback â€” and feel free to use it if it helps you. I'm here to learn and share!

#DataEngineering #GCP #BigQuery #Airflow #Python #DBT #Terraform #MedallionArchitecture #OpenSource #PortfolioProject #CloudData